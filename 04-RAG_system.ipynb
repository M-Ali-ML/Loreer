{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userzero/.local/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-06-22 21:32:03.955302: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-22 21:32:03.987083: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-22 21:32:03.987136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-22 21:32:03.988029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-22 21:32:03.993701: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 21:32:04.753922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch\n",
    "import gc\n",
    "import transformers\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything together - RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    embeddings = torch.load(embeddings_path, map_location='cpu')\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def load_embeddings_model(embedding_model_path, device='cpu'):\n",
    "    model = SentenceTransformer(embedding_model_path, trust_remote_code=True, device=device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def cosine_sim(question, embeddings, embeddings_model):\n",
    "\n",
    "    query_embeddings = embeddings_model.encode(question, batch_size=1, show_progress_bar=False)\n",
    "\n",
    "    cos_sim_score = cos_sim(a=query_embeddings, b=embeddings)[0]\n",
    "\n",
    "    topk = torch.topk(cos_sim_score, k=5)\n",
    "    return topk\n",
    "\n",
    "\n",
    "def retriver(topk, data,min_score, print=False ):\n",
    "    context = []\n",
    "    for k,i in enumerate(topk[1]):\n",
    "        if topk[0][k] >= min_score:\n",
    "            context.append(data.iloc[int(i)][\"text\"])\n",
    "        else:\n",
    "            break\n",
    "    if print:\n",
    "        for chunk in context:\n",
    "            print(\"\\n\\n\\n\")\n",
    "            print(textwrap.fill(chunk, width=120))\n",
    "    return context\n",
    "\n",
    "\n",
    "def load_llm(path, n_ctx,verbose):\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=path,\n",
    "        n_gpu_layers=-1,\n",
    "        n_ctx=n_ctx,\n",
    "        flash_attn=True,\n",
    "        verbose=verbose,\n",
    "        \n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def prompt_extract_from_context(llm, question, context, temperature,max_tokens):\n",
    "\n",
    "    summary_from_context = []\n",
    "    for passage in context:\n",
    "#         extraction_prompt = f\"\"\"\n",
    "# You are an AI assistance based on to this question: {question} \n",
    "# extract related information and summarize them from the given passage, \n",
    "# If the passage doesn't have related information, return [none].\n",
    "# The passage: {passage}.\n",
    "# If the passage doesn't have related information, return [none].\n",
    "#         \"\"\"\n",
    "\n",
    "        extraction_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an AI assistance that extracts related information to a given question and summarize them from the given passage, If the passage doesn't have related information, return [none].\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "The passage: {passage}, the question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "        output = llm(\n",
    "        extraction_prompt,\n",
    "        max_tokens=max_tokens, # set to None to generate up to the end of the context window\n",
    "        stop=[\"Q:\"], # Stop generating just before the model would generate a new question\n",
    "        echo=False,\n",
    "        temperature=temperature,\n",
    "\n",
    "    ) # Generate a completion, can also call create_completion\n",
    "        \n",
    "        summary_from_context.append(output[\"choices\"][0][\"text\"])\n",
    "    return summary_from_context\n",
    "\n",
    "def combine_text(list_of_text, drop_word):\n",
    "    filtered_list_with_indices = [(i, item) for i, item in enumerate(list_of_text) if not re.search(rf\"{drop_word}\", item, re.IGNORECASE)]\n",
    "    \n",
    "    # Separate the indices and items\n",
    "    kept_indices, filtered_list = zip(*filtered_list_with_indices) if filtered_list_with_indices else ([], [])\n",
    "    \n",
    "    # Join the filtered list into a single string with spaces\n",
    "    result = \" \".join(filtered_list)\n",
    "    \n",
    "    return result, kept_indices\n",
    "\n",
    "def get_reference(topk, data, kept_indices, site):\n",
    "    data_index = topk[1][[kept_indices]].tolist()\n",
    "    ref = site + data.iloc[data_index][\"title\"] \n",
    "    return ref\n",
    "\n",
    "def prompt_chat(llm, question, filtered_summary, temperature,max_tokens):\n",
    "    # prompt_chat = f\"\"\"\n",
    "    # You are a wise old Monk that knows all the stories about League of legends, have knowledge of the lore and definitely better than necrit.\n",
    "    # With the following passage: {filtered_summary} from League of Legend wiki, Answer this question:{question} in a very articulate and sophisticated way.\n",
    "    # \"\"\" \n",
    "\n",
    "\n",
    "    prompt_chat = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are loreer, a wise and old AI assistance that knows all the stories about League of legends and its lore, answer the question based on the passage in a very articulate and sophisticated way<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "The passage: {filtered_summary}, the question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    output = llm(\n",
    "    prompt_chat,\n",
    "    max_tokens=max_tokens, # set to None to generate up to the end of the context window\n",
    "    stop=[\"Q:\"], # Stop generating just before the model would generate a new question\n",
    "    echo=False,\n",
    "    temperature=temperature\n",
    "    ) # Generate a completion, can also call create_completion\n",
    "\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "def add_reference(chat_output, ref):\n",
    "    if len(ref) >0:\n",
    "        chat_output = chat_output + \"\\n\\nReferences:\\n \" \n",
    "        for index,link in enumerate(ref):\n",
    "            links = str(index + 1) +\". \" + link + \"\\n\"\n",
    "        chat_output = chat_output + links\n",
    "    return chat_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loreer(question,\n",
    "           llm_model_path,\n",
    "           data_path,\n",
    "           embeddings_path,\n",
    "           embedding_model_path,\n",
    "           n_ctx=6144,\n",
    "           max_tokens=[512,None], # first for extraction summary, second for question answering\n",
    "           print_context=False,\n",
    "           temperature=[0.1, 0.7], # first for extraction summary, second for question answering\n",
    "           verbose=False,\n",
    "           site= \"https://leagueoflegends.fandom.com/wiki/\",\n",
    "           min_score=0.3):\n",
    "    \n",
    "\n",
    "       data = read_data(data_path)\n",
    "       embeddings = load_embeddings(embeddings_path)\n",
    "       embeddings_model = load_embeddings_model(embedding_model_path)\n",
    "       topk = cosine_sim(question, embeddings, embeddings_model)\n",
    "       context = retriver(topk, data, print=print_context,min_score=min_score)\n",
    "       llm = load_llm(llm_model_path,n_ctx,verbose=verbose)\n",
    "       summary_from_context = prompt_extract_from_context(llm, question, context, temperature[0],max_tokens[0])\n",
    "       filtered_summary, kept_indices = combine_text(summary_from_context, \"none\")\n",
    "       ref = get_reference(topk, data, kept_indices, site)\n",
    "       chat_output = prompt_chat(llm, question, filtered_summary, temperature[1],max_tokens[1])\n",
    "       chat_output = add_reference(chat_output, ref, )\n",
    "        \n",
    "    \n",
    "       return chat_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who is Zed main rival in Runeterra\"\n",
    "results, ref = loreer(\n",
    "    question=prompt,\n",
    "    llm_model_path=\"./models/meta-llama-3-8b-instruct.Q4_K_M.gguf\",\n",
    "    data_path=\"./data/chunks_data.csv\",\n",
    "    embeddings_path=\"./data/embeddings_torch.pt\",\n",
    "    embedding_model_path=\"Alibaba-NLP/gte-base-en-v1.5\",\n",
    "    n_ctx=6144,\n",
    "    max_tokens=[512,None], # first for extraction summary, second for question answering\n",
    "    print_context=False,\n",
    "    temperature=[0.05, 0.7], # first for extraction summary, second for question answering\n",
    "    site = \"https://leagueoflegends.fandom.com/wiki/\",\n",
    "    verbose=False,\n",
    "    min_score=0.3\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question=\"Who is Zed main rival in Runeterra\"\n",
    "# llm_model_path=\"./models/meta-llama-3-8b-instruct.Q4_K_M.gguf\"\n",
    "# data_path=\"./data/chunks_data.csv\"\n",
    "# embeddings_path=\"./data/embeddings_torch.pt\"\n",
    "# embedding_model_path=\"Alibaba-NLP/gte-base-en-v1.5\"\n",
    "# n_ctx=4096\n",
    "# max_tokens=[512,None] # first for extraction summary, second for question answering\n",
    "# print_context=False\n",
    "# temperature=[0, 0.7] # first for extraction summary, second for question answering\n",
    "# site = \"https://leagueoflegends.fandom.com/wiki/\"\n",
    "# verbose = False\n",
    "\n",
    "# data = read_data(data_path)\n",
    "# embeddings = load_embeddings(embeddings_path)\n",
    "# embeddings_model = load_embeddings_model(embedding_model_path)\n",
    "# topk = cosine_sim(question, embeddings, embeddings_model)\n",
    "# context = retriver(topk, data)\n",
    "# llm = load_llm(llm_model_path,n_ctx,verbose=verbose)\n",
    "# summary_from_context = prompt_extract_from_context(llm, question, context, temperature[0],max_tokens[0])\n",
    "# filtered_summary, kept_indices = combine_text(summary_from_context, \"none\")\n",
    "# ref = get_reference(topk, data, kept_indices, site)\n",
    "# chat_output = prompt_chat(llm, question, filtered_summary, temperature[1],max_tokens[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
